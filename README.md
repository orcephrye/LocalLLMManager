# Local LLM Manager

Yet another Local LLM Manager. This searches HuggingFace for GGUF/LLama.cpp compatible models. 

It makes it easy to download models and load multiple models with llama.cpp at the same time and 
route models through a simple OpenAI API proxy. This means you can use a tool like 
OpenWeb UI to target a single endpoint and gain access to multiple LLMs.